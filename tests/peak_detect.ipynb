{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n    // Clean up Bokeh references\n    if (id != null && id in Bokeh.index) {\n      Bokeh.index[id].model.document.clear();\n      delete Bokeh.index[id];\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim();\n            if (id in Bokeh.index) {\n              Bokeh.index[id].model.document.clear();\n              delete Bokeh.index[id];\n            }\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\nif (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.signal as signal\n",
    "from scipy.signal import find_peaks, peak_widths, peak_prominences, find_peaks_cwt, argrelextrema\n",
    "from tqdm.notebook import tqdm, trange\n",
    "# bokeh plot settings\n",
    "from bokeh.io import output_notebook, show, save\n",
    "output_notebook()\n",
    "from bokeh.models import ColumnDataSource, HoverTool, Scatter\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.sampledata.autompg import autompg_clean\n",
    "from bokeh.transform import factor_cmap\n",
    "from bokeh.layouts import gridplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Append System Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Path: tests/peak_detect.ipynb\n",
    "import os\n",
    "# find root directory\n",
    "root = os.path.dirname(os.path.abspath('./'))\n",
    "# append root directory to sys.path\n",
    "import sys\n",
    "sys.path.append(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data and pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 101178.39it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 234318.66it/s]\n",
      "100%|██████████| 49/49 [00:00<00:00, 464979.40it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 451404.91it/s]\n",
      "100%|██████████| 57/57 [00:00<00:00, 510844.72it/s]\n",
      "100%|██████████| 57/57 [00:00<00:00, 622592.00it/s]\n",
      "100%|██████████| 47/47 [00:00<00:00, 451103.63it/s]\n",
      "100%|██████████| 46/46 [00:00<00:00, 452906.07it/s]\n",
      "100%|██████████| 81/81 [00:00<00:00, 637408.30it/s]\n",
      "100%|██████████| 81/81 [00:00<00:00, 459727.50it/s]\n",
      "100%|██████████| 57/57 [00:00<00:00, 356828.85it/s]\n",
      "100%|██████████| 56/56 [00:00<00:00, 489335.47it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 2179.94it/s]\n",
      "100%|██████████| 49/49 [00:00<00:00, 1596.14it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 1574.87it/s]\n",
      "100%|██████████| 57/57 [00:00<00:00, 3114.66it/s]\n",
      "/home/gakkispy/Code/overlap_project/tools/read_mzxml.py:64: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  point_list = np.array(point_list)\n",
      "100%|██████████| 57/57 [00:00<00:00, 3061.81it/s]\n",
      "100%|██████████| 47/47 [00:00<00:00, 1612.07it/s]\n",
      "100%|██████████| 46/46 [00:00<00:00, 1595.25it/s]\n",
      "100%|██████████| 81/81 [00:00<00:00, 5721.05it/s]\n",
      "100%|██████████| 81/81 [00:00<00:00, 6259.23it/s]\n",
      "100%|██████████| 57/57 [00:00<00:00, 2965.02it/s]\n",
      "100%|██████████| 56/56 [00:00<00:00, 2992.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from tools.get_files import GetFiles\n",
    "from tools.read_mzxml import read_mzxml\n",
    "\n",
    "\n",
    "file_finder = GetFiles('/mnt/d/Data/LTQ数据/', 'mzXML')\n",
    "file_list = file_finder.get_files()\n",
    "xml_data_list = []\n",
    "filename_list = []\n",
    "for file in file_list:\n",
    "    filename_list.append(file.split('/')[-1])\n",
    "    xml_data_list.append([read_mzxml(file, env='Production')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mass_spectrum(data):\n",
    "    \"\"\"\n",
    "    Get the mass spectrum of a specific mass from the data.\n",
    "    \"\"\"\n",
    "    spectrum_list = []\n",
    "    for i in range(len(data)):\n",
    "        spectrum = pd.DataFrame({'mz': data[i][0::2], 'intensity': data[i][1::2]},)\n",
    "        spectrum_list.append(spectrum)\n",
    "    return spectrum_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gakkispy/miniconda3/envs/findpeaks/lib/python3.7/site-packages/bokeh/io/saving.py:142: UserWarning: save() called but no resources were supplied and output_file(...) was never called, defaulting to resources.CDN\n",
      "  warn(\"save() called but no resources were supplied and output_file(...) was never called, defaulting to resources.CDN\")\n",
      "/home/gakkispy/miniconda3/envs/findpeaks/lib/python3.7/site-packages/bokeh/io/saving.py:154: UserWarning: save() called but no title was supplied and output_file(...) was never called, using default title 'Bokeh Plot'\n",
      "  warn(\"save() called but no title was supplied and output_file(...) was never called, using default title 'Bokeh Plot'\")\n"
     ]
    }
   ],
   "source": [
    "file_data_dict = dict(zip(filename_list, xml_data_list))\n",
    "spectrums_list = []\n",
    "for key, value in file_data_dict.items():\n",
    "    point_list = value[0][0]\n",
    "    spectrum_list = get_mass_spectrum(point_list)\n",
    "    spectrums_list.append(spectrum_list)\n",
    "\n",
    "file_spectrum_dict = dict(zip(filename_list, spectrums_list))\n",
    "\n",
    "for key, value in file_spectrum_dict.items():\n",
    "    for id, spectrum in enumerate(value):\n",
    "        p = figure(title=key, x_axis_label='m/z', y_axis_label='intensity')\n",
    "        p.line(spectrum['mz'], spectrum['intensity'], line_width=2)\n",
    "        save_dir = '/mnt/d/Data/LTQ数据/' + os.path.splitext(key)[0] + '/'\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        save(p, filename='{}.html'.format(save_dir + str(id)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:00<00:00, 94608.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# read data from csv file\n",
    "data_dir = os.path.join(root, 'data/')\n",
    "from tools.get_files import GetFiles\n",
    "file_finder = GetFiles(data_dir, '.csv')\n",
    "file_list = file_finder.get_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# read csv files\n",
    "def read_csv(file):\n",
    "    df = pd.read_csv(file, )\n",
    "    df.dropna()\n",
    "    filename = os.path.basename(os.path.splitext(file)[0])\n",
    "    return df, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "data_list, filename_list = [], []\n",
    "for file in tqdm(file_list):\n",
    "    data, filename = read_csv(file)\n",
    "    data = data[['x','y']]\n",
    "    data_list.append(data)\n",
    "    filename_list.append(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Content -- Data process and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoise filter definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# plot data with denoise function\n",
    "def plot_after_filted(filter, data, title:str):\n",
    "    if filter.__name__ == 'denoise':\n",
    "        p = figure(plot_width=800, plot_height=400, title=title + ' with uv_filter')\n",
    "        filted_data = filter(data['y'], 100)\n",
    "    else:\n",
    "        p = figure(plot_width=800, plot_height=400, title=title + ' with ' + filter.__name__)\n",
    "        filted_data = filter(data['y'])\n",
    "    p.line(data['x'], data['y'], line_dash = (4, 4), line_width=2, color='skyblue')\n",
    "    p.line(data['x'], filted_data, line_width=2, color='red')\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "from filter import *\n",
    "uv_denoiser = Denoiser()\n",
    "\n",
    "# define filter function list\n",
    "filter_list = [conv_filter, wavelet_noising, savgol_filter, signal.medfilt, signal.wiener, uv_denoiser.denoise]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data filter and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "plot_list = []\n",
    "for i in trange(len(data_list)):\n",
    "    for filter in tqdm(filter_list):\n",
    "        temp_p = plot_after_filted(filter, data_list[i], filename_list[i])\n",
    "        plot_list.append(temp_p)\n",
    "\n",
    "grid = gridplot(plot_list, ncols=3, plot_width=400, plot_height=400)\n",
    "show(grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define peak find function by using peakutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "def plot_peak(p, peak_index, peak_height, data, title:str):\n",
    "    p.scatter(data['x'][peak_index], peak_height, size=5, color='darkcyan')\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import peakutils\n",
    "\n",
    "peak_plot_list = []\n",
    "for i in trange(len(data_list)):\n",
    "    for filter in tqdm(filter_list):\n",
    "        temp_p = plot_after_filted(filter, data_list[i], filename_list[i])\n",
    "        peak_index = peakutils.indexes(data_list[i]['y'], thres=0.5, min_dist=100)\n",
    "        peak_height = data_list[i]['y'][peak_index]\n",
    "        temp_peak_p = plot_peak(temp_p, peak_index, peak_height, data_list[i], filename_list[i])\n",
    "        peak_plot_list.append(temp_peak_p)\n",
    "\n",
    "grid_peak = gridplot(peak_plot_list, ncols=3, plot_width=400, plot_height=400)\n",
    "show(grid_peak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "## test gaussian fit\n",
    "for i in trange(len(data_list)):\n",
    "    test = peakutils.gaussian_fit(data_list[i]['x'], data_list[i]['y'], False)\n",
    "    peak_index = peakutils.indexes(data_list[i]['y'], thres=0.5, min_dist=100)\n",
    "    peak_height = data_list[i]['y'][peak_index]\n",
    "    gaussian = peakutils.gaussian(data_list[i]['x'], test[0], test[1], test[2])\n",
    "    p = figure(plot_width=800, plot_height=400, title=filename_list[i] + ' with gaussian fit')\n",
    "    p.line(data_list[i]['x'], data_list[i]['y'], line_dash = (4, 4), line_width=2, color='skyblue')\n",
    "    p.line(data_list[i]['x'], gaussian, line_width=2, color='red')\n",
    "    show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary gaussian fit definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "def guassian(x, *params):\n",
    "    num_func = int(len(params)/3)\n",
    "\n",
    "    y_list = []\n",
    "\n",
    "    for i in range(num_func):\n",
    "        y = np.zeros_like(x)\n",
    "        param_range = list(range(3*i, 3*(i+1), 1))\n",
    "        amp = params[int(param_range[0])]\n",
    "        ctr = params[int(param_range[1])]\n",
    "        wid = params[int(param_range[2])]\n",
    "        y += amp * np.exp(-(x - ctr)**2 / wid)\n",
    "        y_list.append(y)\n",
    "    \n",
    "    y_sum = np.zeros_like(x)\n",
    "    for i in y_list:\n",
    "        y_sum += i\n",
    "\n",
    "    y_sum += params[-1]\n",
    "\n",
    "    return y_sum\n",
    "\n",
    "def peaks_fit(x, *params):\n",
    "    num_func = int(len(params)/3)\n",
    "\n",
    "    y_list = []\n",
    "\n",
    "    for i in range(num_func):\n",
    "        y = np.zeros_like(x)\n",
    "        param_range = list(range(3*i, 3*(i+1), 1))\n",
    "        amp = params[int(param_range[0])]\n",
    "        ctr = params[int(param_range[1])]\n",
    "        wid = params[int(param_range[2])]\n",
    "        y += amp * np.exp(-(x - ctr)**2 / wid) + params[-1]\n",
    "        y_list.append(y)\n",
    "    return y_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define peak find function by using findpeaks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from findpeaks import findpeaks\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def plot_peak_fit(p, peak_index, peak_height, data, title:str):\n",
    "    p.scatter(data['x'][peak_index], peak_height, size=5, color='darkcyan')\n",
    "    return p\n",
    "\n",
    "def plot_valley_fit(p, valley_index, valley_height, data, title:str):\n",
    "    p.star(data['x'][valley_index], valley_height, size=5, color='darkcyan')\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle peak fit reuslt\n",
    "def peak_fit_result(result, data):\n",
    "    df = result['df']\n",
    "    df['x'] = data['x']\n",
    "    peak_index= df[lambda df: df['peak'] == True]\n",
    "    highest_peak = df.loc[peak_index.index.tolist()]['y'].max()\n",
    "    peak_index_list = peak_index[lambda peak_index: peak_index['y'] > .1 * highest_peak].index.tolist()\n",
    "    valley_index = df[lambda df: df['valley'] == True]\n",
    "    valley_index_list = valley_index[lambda valley_index: valley_index['y'] > .1 * highest_peak].index.tolist()\n",
    "    print('peak index: ', peak_index_list)\n",
    "    print('valley index: ', valley_index_list)\n",
    "    return peak_index_list, valley_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb10a84e3ff54da6b3c9748a03140305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no peak found in  手动分析[2022.10.18-08.30.32].mzXML\n",
      "no peak found in  离子阱测试dc20中中0.05ms.mzXML\n",
      "no peak found in  离子阱测试dc20中中0.05ms下午.mzXML\n",
      "no peak found in  离子阱测试dc20中中0.5ms.mzXML\n",
      "no peak found in  离子阱测试dc20中中0.5ms下午.mzXML\n",
      "no peak found in  离子阱测试dc20中中5ms.mzXML\n",
      "no peak found in  离子阱测试dc20中中5ms下午.mzXML\n",
      "no peak found in  离子阱测试dc20高中0.5m.mzXML\n",
      "no peak found in  离子阱测试dc20高中0.5m下午.mzXML\n",
      "no peak found in  离子阱测试dc70中中0.5mm.mzXML\n",
      "no peak found in  离子阱测试dc70中中0.5ms下午.mzXML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div class=\"bk-root\" id=\"7b23b1b2-25e1-4f6f-8aa3-4c9138e7736f\" data-root-id=\"102836\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function embed_document(root) {\n  const docs_json = {\"f5f46a78-12b8-45e0-8a00-49a460b3bd39\":{\"defs\":[],\"roots\":{\"references\":[{\"attributes\":{\"children\":[{\"id\":\"102835\"},{\"id\":\"102833\"}]},\"id\":\"102836\",\"type\":\"Column\"},{\"attributes\":{},\"id\":\"102834\",\"type\":\"ProxyToolbar\"},{\"attributes\":{},\"id\":\"102833\",\"type\":\"GridBox\"},{\"attributes\":{\"toolbar\":{\"id\":\"102834\"},\"toolbar_location\":\"above\"},\"id\":\"102835\",\"type\":\"ToolbarBox\"}],\"root_ids\":[\"102836\"]},\"title\":\"Bokeh Application\",\"version\":\"2.4.3\"}};\n  const render_items = [{\"docid\":\"f5f46a78-12b8-45e0-8a00-49a460b3bd39\",\"root_ids\":[\"102836\"],\"roots\":{\"102836\":\"7b23b1b2-25e1-4f6f-8aa3-4c9138e7736f\"}}];\n  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n  }\n  if (root.Bokeh !== undefined) {\n    embed_document(root);\n  } else {\n    let attempts = 0;\n    const timer = setInterval(function(root) {\n      if (root.Bokeh !== undefined) {\n        clearInterval(timer);\n        embed_document(root);\n      } else {\n        attempts++;\n        if (attempts > 100) {\n          clearInterval(timer);\n          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n        }\n      }\n    }, 10, root)\n  }\n})(window);",
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "102836"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_fp_list = []\n",
    "for i in trange(len(data_list)):\n",
    "    # peak_ctr = peakutils.centroid(data_list[i]['x'], data_list[i]['y'])\n",
    "    fp = findpeaks(method='topology', interpolate=10, lookahead=100, limit=5)\n",
    "    try:\n",
    "        fp_result = fp.fit(data_list[i]['y'])\n",
    "        peak_index, valley_index = peak_fit_result(fp_result, data_list[i])\n",
    "        peak_height = data_list[i]['y'][peak_index]\n",
    "        p = figure(plot_width=800, plot_height=400, title=filename_list[i] + ' with peak fit')\n",
    "        p.line(data_list[i]['x'], data_list[i]['y'], line_dash = (4, 4), line_width=2, color='skyblue')\n",
    "        p = plot_peak_fit(p, peak_index, peak_height, data_list[i], filename_list[i])\n",
    "        p = plot_valley_fit(p, valley_index, data_list[i]['y'][valley_index], data_list[i], filename_list[i])\n",
    "        grid_fp_list.append(p)\n",
    "    except:\n",
    "        print('no peak found in ', filename_list[i])\n",
    "grid_fp = gridplot(grid_fp_list, ncols=3, plot_width=400, plot_height=400)\n",
    "show(grid_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curve fit with gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d910011c00f34338bc086017a785da65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1134/2836469173.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mexp_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExponentialModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'exp_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mgauss_main\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gauss_main_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgauss_main\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# curve fit\n",
    "from lmfit.models import GaussianModel, LinearModel, ExponentialModel\n",
    "grid_cf_list = []\n",
    "for i in trange(len(data_list)):\n",
    "    exp_mod = ExponentialModel(prefix='exp_')\n",
    "    pars = exp_mod.guess(data_list[i]['y'], x=data_list[i]['x'])\n",
    "    gauss_main = GaussianModel(prefix='gauss_main_')\n",
    "    pars.update(gauss_main.make_params())\n",
    "    fp = findpeaks(method='topology', interpolate=10, lookahead=100, limit=5)\n",
    "    try:\n",
    "        fp_result = fp.fit(data_list[i]['y'])\n",
    "        peak_index, valley_index = peak_fit_result(fp_result, data_list[i])\n",
    "\n",
    "        pars['gauss_main_center'].set(data_list[i]['x'][peak_index[0]], min=data_list[i]['x'][peak_index[0]]-50, max=data_list[i]['x'][peak_index[0]]+50)\n",
    "        pars['gauss_main_sigma'].set(5, min=0.1, max=50)\n",
    "        pars['gauss_main_amplitude'].set(10, min=0, max=150)\n",
    "        p = figure(plot_width=800, plot_height=400, title=filename_list[i] + ' with curve fit')\n",
    "        p.line(data_list[i]['x'], data_list[i]['y'], line_dash = (4, 4), line_width=2, color='skyblue')\n",
    "        if len(peak_index) > 1:\n",
    "            gauss_side = GaussianModel(prefix='gauss_side_')\n",
    "            pars.update(gauss_side.make_params())\n",
    "            pars['gauss_side_center'].set(data_list[i]['x'][peak_index[1]], min=data_list[i]['x'][peak_index[1]]-50, max=data_list[i]['x'][peak_index[1]]+50)\n",
    "            pars['gauss_side_sigma'].set(5, min=0.1, max=50)\n",
    "            pars['gauss_side_amplitude'].set(10, min=0, max=150)\n",
    "            mod = exp_mod + gauss_main + gauss_side\n",
    "            out = mod.fit(data_list[i]['y'], pars, x=data_list[i]['x'])\n",
    "            comps = out.eval_components(x=data_list[i]['x'])\n",
    "            p.line(data_list[i]['x'], comps['gauss_side_'], line_width=2, color='lightcoral')\n",
    "        else:\n",
    "            mod = exp_mod + gauss_main\n",
    "            out = mod.fit(data_list[i]['y'], pars, x=data_list[i]['x'])\n",
    "            comps = out.eval_components(x=data_list[i]['x'])\n",
    "\n",
    "        # fwhm = [out.result.params['gauss_main_fwhm'].value, out.result.params['gauss_side_fwhm'].value]\n",
    "        # print(fwhm)\n",
    "        # break\n",
    "        print(out.fit_report())\n",
    "        p.line(data_list[i]['x'], comps['gauss_main_'], line_width=2, color='darkcyan')\n",
    "        peak_height = data_list[i]['y'][peak_index]\n",
    "        p.line(data_list[i]['x'], data_list[i]['y'], line_dash = (4, 4), line_width=2, color='skyblue')\n",
    "        p = plot_peak_fit(p, peak_index, peak_height, data_list[i], filename_list[i])\n",
    "        p = plot_valley_fit(p, valley_index, data_list[i]['y'][valley_index], data_list[i], filename_list[i])\n",
    "        grid_cf_list.append(p)\n",
    "    except:\n",
    "        print('no peak found in ', filename_list[i])\n",
    "grid_cf = gridplot(grid_cf_list, ncols=3, plot_width=400, plot_height=400)\n",
    "show(grid_cf)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curve fit with Lorentzian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curve fit\n",
    "from lmfit.models import GaussianModel, LinearModel, ExponentialModel, LorentzianModel\n",
    "grid_cf_list = []\n",
    "for i in trange(len(data_list)):\n",
    "    exp_mod = ExponentialModel(prefix='exp_')\n",
    "    pars = exp_mod.guess(data_list[i]['y'], x=data_list[i]['x'])\n",
    "    loren_main = LorentzianModel(prefix='loren_main_')\n",
    "    pars.update(loren_main.make_params())\n",
    "    fp = findpeaks(method='topology', interpolate=10, lookahead=100, limit=5)\n",
    "    try:\n",
    "        fp_result = fp.fit(data_list[i]['y'])\n",
    "        peak_index, valley_index = peak_fit_result(fp_result, data_list[i])\n",
    "        gaussian_fit_result = peakutils.gaussian_fit(data_list[i]['x'], data_list[i]['y'], False)\n",
    "        pars['loren_main_center'].set(gaussian_fit_result[1], min=data_list[i]['x'][peak_index[0]]-50, max=data_list[i]['x'][peak_index[0]]+50)\n",
    "        pars['loren_main_sigma'].set(gaussian_fit_result[2], min=0.1, max=50)\n",
    "        pars['loren_main_amplitude'].set(gaussian_fit_result[0], min=0, max=150)\n",
    "        mod = exp_mod + loren_main\n",
    "        out = mod.fit(data_list[i]['y'], pars, x=data_list[i]['x'])\n",
    "        comps = out.eval_components(x=data_list[i]['x'])\n",
    "        print(out.fit_report())\n",
    "        p = figure(plot_width=800, plot_height=400, title=filename_list[i] + ' with curve fit')\n",
    "        p.line(data_list[i]['x'], data_list[i]['y'], line_dash = (4, 4), line_width=2, color='skyblue')\n",
    "        p.line(data_list[i]['x'], out.best_fit, line_width=2, color='darkcyan')\n",
    "        peak_height = data_list[i]['y'][peak_index]\n",
    "        p.line(data_list[i]['x'], data_list[i]['y'], line_dash = (4, 4), line_width=2, color='skyblue')\n",
    "        p = plot_peak_fit(p, peak_index, peak_height, data_list[i], filename_list[i])\n",
    "        p = plot_valley_fit(p, valley_index, data_list[i]['y'][valley_index], data_list[i], filename_list[i])\n",
    "        grid_cf_list.append(p)\n",
    "    except:\n",
    "        print('no peak found in ', filename_list[i])\n",
    "grid_cf = gridplot(grid_cf_list, ncols=3, plot_width=400, plot_height=400)\n",
    "show(grid_cf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('findpeaks')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "988dc20507c9b8666a77143fe2ea187e26c55df0acae05133eb0d05f63ed1f23"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
